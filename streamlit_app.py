import streamlit as st
from src import WebEnhancedLLM, SearchEngineConfig
from dotenv import load_dotenv
import os
import asyncio
from typing import AsyncGenerator, Dict, List

# åˆå§‹åŒ–é…ç½®
load_dotenv()
st.set_page_config(page_title="Web search with Any LLM ", layout="wide")

class ChatApp:
    def __init__(self):
        self.config = self._load_config()
        self.llm = self._init_llm()
        self.setup_ui()

    def _load_config(self):
        """åŠ è½½æˆ–åˆå§‹åŒ–é…ç½®"""
        def safe_int(value: str, default: int) -> int:
            try:
                return int(value.split('#')[0].strip())
            except (ValueError, AttributeError):
                return default

        if "config" not in st.session_state:
            st.session_state.config = {
                "model": os.getenv('MODEL_NAME', ''),
                "api_key": os.getenv("MODEL_API_KEY", ""),
                "api_base": os.getenv('API_BASE_URL', ''),
                "depth": safe_int(os.getenv('DEPTH'), 1),
                "width": safe_int(os.getenv('WIDTH'), 3),
                "max_final_results": safe_int(os.getenv('MAX_SEARCH_RESULTS'), 15),
                "engines": {
                    'baidu': {
                        'enabled': os.getenv('BAIDU_ENABLED', 'true').lower() == 'true',
                        'max_results': safe_int(os.getenv('BAIDU_MAX_RESULTS'), 5)
                    },
                    'duckduckgo': {
                        'enabled': os.getenv('DUCKDUCKGO_ENABLED', 'false').lower() == 'true',
                        'max_results': safe_int(os.getenv('DUCKDUCKGO_MAX_RESULTS'), 5)
                    },
                    'bocha': {
                        'enabled': os.getenv('BOCHA_ENABLED', 'false').lower() == 'true',
                        'max_results': safe_int(os.getenv('BOCHA_MAX_RESULTS'), 3),
                        'api_key': os.getenv('BOCHA_API_KEY', '')
                    },
                    'tavily': {
                        'enabled': os.getenv('TAVILY_ENABLED', 'false').lower() == 'true',
                        'max_results': safe_int(os.getenv('TAVILY_MAX_RESULTS'), 5),
                        'api_key': os.getenv('TAVILY_API_KEY', '')
                    }
                }
            }
        return st.session_state.config

    def _show_config_ui(self):
        """æ˜¾ç¤ºé…ç½®ç•Œé¢"""
        with st.sidebar.expander("âš™ï¸ å¼•æ“é…ç½®"):
            # æ¨¡å‹é…ç½®
            self.config['model'] = st.text_input("æ¨¡å‹åç§°", value=self.config['model'])
            self.config['api_key'] = st.text_input("API Key", value=self.config['api_key'], type="password")
            self.config['api_base'] = st.text_input("APIåœ°å€", value=self.config['api_base'])
            
            # æ–°å¢æœç´¢å‚æ•°é…ç½®
            st.subheader("æœç´¢å‚æ•°")
            self.config['depth'] = st.number_input("æœç´¢æ·±åº¦", min_value=1, max_value=5, value=self.config['depth'])
            self.config['width'] = st.number_input("æœç´¢å®½åº¦", min_value=1, max_value=5, value=self.config['width'])
            self.config['max_final_results'] = st.number_input("æœ€å¤§ç»“æœæ•°", min_value=1, max_value=20, value=self.config['max_final_results'])
            
            # å¼•æ“é…ç½®
            for engine, config in self.config['engines'].items():
                st.subheader(f"{engine}é…ç½®")
                config['enabled'] = st.checkbox(f"å¯ç”¨{engine}", value=config['enabled'])
                if config['enabled']:
                    config['max_results'] = st.number_input(
                        f"{engine}æœ€å¤§ç»“æœæ•°", 
                        min_value=1, max_value=10, value=config['max_results']
                    )
                    if engine in ['tavily', 'bocha']:  # éœ€è¦API Keyçš„å¼•æ“
                        config['api_key'] = st.text_input(
                            f"{engine} API Key", 
                            value=config.get('api_key', ''), 
                            type="password"
                        )
        
            if st.button("ä¿å­˜é…ç½®"):
                self.llm = self._init_llm()
                st.success("é…ç½®å·²æ›´æ–°ï¼")

    def _init_llm(self) -> WebEnhancedLLM:
        """åˆå§‹åŒ–LLMå®ä¾‹"""
        engine_config = {
            name: SearchEngineConfig(
                name,
                max_results=config['max_results'],
                api_key=config.get('api_key', ''),
                enabled=config['enabled']
            )
            for name, config in self.config['engines'].items()
        }
        
        return WebEnhancedLLM(
            model=self.config['model'],
            search_engine=[name for name, cfg in self.config['engines'].items() if cfg['enabled']],
            language='zh',
            api_key=self.config['api_key'],
            api_base_url=self.config['api_base'],
            engine_config=engine_config,
            depth=self.config['depth'],
            width=self.config['width']
        )

    def setup_ui(self):
        """è®¾ç½®ç”¨æˆ·ç•Œé¢"""
        st.title("ğŸ” Web search with Any LLM")
        self._show_config_ui()  # æ·»åŠ é…ç½®ç•Œé¢
        st.caption("è½»é‡çš„æ·±åº¦è”ç½‘æœç´¢")
        
        # æ·»åŠ é‡æ–°å¯¹è¯æŒ‰é’®
        if st.button("ğŸ”„ é‡æ–°å¯¹è¯"):
            st.session_state.messages = [
                {"role": "assistant", "content": "ä½ å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"}
            ]
            st.rerun()
        
        if "messages" not in st.session_state:
            st.session_state.messages = [
                {"role": "assistant", "content": "ä½ å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"}
            ]

        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for msg in st.session_state.messages:
            st.chat_message(msg["role"]).write(msg["content"])

    async def process_query(self, query: str) -> AsyncGenerator[str, None]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆå“åº”"""
        
        # åˆå§‹åŒ–å˜é‡
        import json
        intent_content = ""
        reasoning_content = ""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": query})
        reasoning_emitted = False
        # å‡†å¤‡å¯¹è¯ä¸Šä¸‹æ–‡
        messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"}]
        messages.extend([{"role": msg["role"], "content": msg["content"]} 
                        for msg in st.session_state.messages[:-1]])
        print(messages)
        think_mode=False
        # æµå¼å¤„ç†å“åº”
        async for chunk in self.llm.web_search(query, context=messages, tools={'è”ç½‘æœç´¢'}):
            reasoning_content=''
            # æ£€æŸ¥æ˜¯å¦æœ‰ stop_reason å­—æ®µ
            if not hasattr(chunk, 'choices') or not chunk.choices:
                continue
         
            if hasattr(chunk.choices[0].delta, 'reasoning_content')and chunk.choices[0].delta.reasoning_content:
                        reasoning_content += chunk.choices[0].delta.reasoning_content
                        if not reasoning_emitted :
                            start_label = "<--------æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹------>\n"
                            yield f"{start_label}"
                            reasoning_emitted= True
                        md_content = f"{chunk.choices[0].delta.reasoning_content}"
                        yield md_content
            if chunk.choices[0].delta.content:
                        if chunk.choices[0].delta.content=="<think>":
                                think_mode=True
                                start_label = "<-------æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹----->\n"
                                yield f"{start_label}"
                        elif chunk.choices[0].delta.content == "</think>":
                                think_mode = False
                        elif think_mode:  # åªæœ‰åœ¨thinkæ¨¡å¼ä¸‹çš„éæ ‡ç­¾å†…å®¹æ‰ç‰¹æ®Šå¤„ç†
                                yield f"{chunk.choices[0].delta.content}"
                        intent_content+= chunk.choices[0].delta.content
        keywords,action_name=self.llm.extract_keywords_response(intent_content)
        
        web_search_mes=messages.copy()
        if keywords and action_name =='search':
            # 2.æœç´¢
            use_label=f"\n<----ä½¿ç”¨å·¥å…·: è”ç½‘æœç´¢----->"
            yield f"{use_label}"
            search_results,ref= await self.llm.search_with_context(keywords,enable_reflection=True)
            
            # 3.ç”Ÿæˆæœç´¢ç»“æœçš„æ€»ç»“æç¤º        
            summary_prompt = self.llm.get_summary_prompt(keywords,search_results)
            
            # ç®€åŒ–æœç´¢ç»“æœå±•ç¤º
            search_results_display = ""
            for i, result in enumerate(search_results, 1):
                search_results_display += f"""
{i}. **{result.get('title', 'æ— æ ‡é¢˜')}**  
ğŸ”— {result.get('href', '')}\n
"""
            yield search_results_display
            print('æœç´¢ç»“æœ:',search_results)
            web_search_mes=messages[-2:].copy() if len(messages) >2 else messages.copy()
            web_search_mes[-1]={
                    "role": "user",
                    "content": summary_prompt
                }
        think_mode=False
        reasoning_emitted=False
        async for chunk in self.llm.call_llm_stream(messages=web_search_mes): 
                        if not hasattr(chunk, 'choices') or not chunk.choices:
                                    continue
                        elif hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content :
                            reasoning_emitted=True
                            html_content = chunk.choices[0].delta.reasoning_content 
                            yield f"{json.dumps(html_content, ensure_ascii=False)}\n"
                            
                        elif hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content :
                                if reasoning_emitted:
                                    reasoning_emitted=False
                                    think_mode = False
                                    label="\n-----æ€è€ƒç»“æŸ:å‡†å¤‡è¾“å‡ºç»“æœ-----"
                                    yield f"{label}"
                               
                            
                      
                                elif chunk.choices[0].delta.content=="<think>":
                                    think_mode=True
                                    labelx="\n<-----æ ¹æ®æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œè¿›è¡Œå¦‚ä¸‹åˆ†æå’Œæ¢³ç†----->"
                                    yield f"{labelx}\n"
                            
                                elif chunk.choices[0].delta.content == "</think>":
                                    label="\n<-----æ€è€ƒç»“æŸ:å‡†å¤‡è¾“å‡ºç»“æœ----->"
                                    yield f"{label}"
                                    think_mode = False
                                elif  not think_mode  or not  reasoning_emitted:   
                                     yield {"type": "full_results", "data": chunk.choices[0].delta.content}
                     
                        

    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        if query := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            st.chat_message("user").write(query)
            
            # åˆ›å»ºå ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
            response_placeholder = st.empty()
            full_response = ""
            save_content=""
            # å¤„ç†å¹¶æ˜¾ç¤ºå“åº”
            async def stream_response():
                nonlocal full_response
                nonlocal save_content
                async for chunk in self.process_query(query):
                    #full_response += chunk
                    if isinstance(chunk, dict):
                        if chunk.get("type") == "full_results":
                            print(chunk["data"])
                            save_content += chunk["data"]  # ä¿å­˜å®Œæ•´ç»“æœ
                            full_response += chunk["data"] 
                       
                    else:
                        #print(chunk)
                        full_response += chunk
                    response_placeholder.markdown(full_response)
                print("save_content:", save_content)
                # æ·»åŠ å®Œæ•´å“åº”åˆ°å†å²
                st.session_state.messages.append({"role": "assistant", "content":  save_content})
            
            asyncio.run(stream_response())

if __name__ == "__main__":
    app = ChatApp()
    app.run()