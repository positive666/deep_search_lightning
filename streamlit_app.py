import streamlit as st
from src import WebEnhancedLLM, SearchEngineConfig
from dotenv import load_dotenv
import os
import asyncio
from typing import AsyncGenerator, Dict, List

# åˆå§‹åŒ–é…ç½®
load_dotenv(override=True)
st.set_page_config(page_title="Web search with Any LLM ", layout="wide")
# # ä¸´æ—¶æµ‹è¯•ç”¨ç»å¯¹è·¯å¾„
# åœ¨process_queryæ–¹æ³•å¼€å¤´ä¸´æ—¶æµ‹è¯•

#logo_path = os.path.abspath(os.path.join(os.path.dirname(__file__), 'assets', f'baidu.png'))
# print(f"Absolute logo path: {logo_path}")  # æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®
class ChatApp:
    def __init__(self):
        #assets_dir = os.path.join(os.path.dirname(__file__), 'assets')
    #     self.ENGINE_LOGOS = {
    #     engine: f'assets/{engine}.png'
    #     for engine in ['baidu', 'duckduckgo', 'bocha', 'tavily']
    # }
        self.config = self._load_config()
        self.llm = self._init_llm()
        self.setup_ui()

    def _load_config(self):
        """åŠ è½½æˆ–åˆå§‹åŒ–é…ç½®"""
        def safe_int(value: str, default: int) -> int:
            try:
                return int(value.split('#')[0].strip())
            except (ValueError, AttributeError):
                return default

        if "config" not in st.session_state:
            st.session_state.config = {
                "model": os.getenv('MODEL_NAME', ''),
                "api_key": os.getenv("MODEL_API_KEY", ""),
                "api_base": os.getenv('API_BASE_URL', ''),
                "depth": safe_int(os.getenv('DEPTH'), 1),
                "width": safe_int(os.getenv('WIDTH'), 3),
                "max_final_results": safe_int(os.getenv('MAX_SEARCH_RESULTS'), 15),
                "engines": {
                    'baidu': {
                        'enabled': os.getenv('BAIDU_ENABLED', 'true').lower() == 'true',
                        'max_results': safe_int(os.getenv('BAIDU_MAX_RESULTS'), 5)
                    },
                    'duckduckgo': {
                        'enabled': os.getenv('DUCKDUCKGO_ENABLED', 'false').lower() == 'true',
                        'max_results': safe_int(os.getenv('DUCKDUCKGO_MAX_RESULTS'), 5)
                    },
                    'bocha': {
                        'enabled': os.getenv('BOCHA_ENABLED', 'false').lower() == 'true',
                        'max_results': safe_int(os.getenv('BOCHA_MAX_RESULTS'), 3),
                        'api_key': os.getenv('BOCHA_API_KEY', '')
                    },
                    'tavily': {
                        'enabled': os.getenv('TAVILY_ENABLED', 'false').lower() == 'true',
                        'max_results': safe_int(os.getenv('TAVILY_MAX_RESULTS'), 5),
                        'api_key': os.getenv('TAVILY_API_KEY', '')
                    }
                }
            }
        return st.session_state.config

    def _show_config_ui(self):
        """æ˜¾ç¤ºé…ç½®ç•Œé¢"""
        with st.sidebar.expander("âš™ï¸ å¼•æ“é…ç½®"):
            # æ¨¡å‹é…ç½®
            self.config['model'] = st.text_input("æ¨¡å‹åç§°", value=self.config['model'])
            self.config['api_key'] = st.text_input("API Key", value=self.config['api_key'], type="password")
            self.config['api_base'] = st.text_input("APIåœ°å€", value=self.config['api_base'])
            
            # æ–°å¢æœç´¢å‚æ•°é…ç½®
            st.subheader("æœç´¢å‚æ•°")
            self.config['depth'] = st.number_input("æœç´¢æ·±åº¦", min_value=1, max_value=5, value=self.config['depth'])
            self.config['width'] = st.number_input("æœç´¢å®½åº¦", min_value=1, max_value=5, value=self.config['width'])
            self.config['max_final_results'] = st.number_input("æœ€å¤§ç»“æœæ•°", min_value=1, max_value=20, value=self.config['max_final_results'])
            
            # å¼•æ“é…ç½®
            for engine, config in self.config['engines'].items():
                st.subheader(f"{engine}é…ç½®")
                config['enabled'] = st.checkbox(f"å¯ç”¨{engine}", value=config['enabled'])
                if config['enabled']:
                    config['max_results'] = st.number_input(
                        f"{engine}æœ€å¤§ç»“æœæ•°", 
                        min_value=1, max_value=10, value=config['max_results']
                    )
                    if engine in ['tavily', 'bocha']:  # éœ€è¦API Keyçš„å¼•æ“
                        config['api_key'] = st.text_input(
                            f"{engine} API Key", 
                            value=config.get('api_key', ''), 
                            type="password"
                        )
        
            if st.button("ä¿å­˜é…ç½®"):
                self.llm = self._init_llm()
                st.success("é…ç½®å·²æ›´æ–°ï¼")

    def _init_llm(self) -> WebEnhancedLLM:
        """åˆå§‹åŒ–LLMå®ä¾‹"""
        engine_config = {
            name: SearchEngineConfig(
                name,
                max_results=config['max_results'],
                api_key=config.get('api_key', ''),
                enabled=config['enabled']
            )
            for name, config in self.config['engines'].items()
        }
        
        return WebEnhancedLLM(
            model=self.config['model'],
            search_engine=[name for name, cfg in self.config['engines'].items() if cfg['enabled']],
            language='zh',
            api_key=self.config['api_key'],
            api_base_url=self.config['api_base'],
            engine_config=engine_config,
            depth=self.config['depth'],
            width=self.config['width']
        )

    def setup_ui(self):
        """è®¾ç½®ç”¨æˆ·ç•Œé¢"""
        st.title("ğŸ” Web search with Any LLM")
        self._show_config_ui()  # æ·»åŠ é…ç½®ç•Œé¢
        st.caption("è½»é‡çš„æ·±åº¦è”ç½‘æœç´¢")
        
        # æ·»åŠ é‡æ–°å¯¹è¯æŒ‰é’®
        if st.button("ğŸ”„ é‡æ–°å¯¹è¯"):
            st.session_state.messages = [
                {"role": "assistant", "content": "ä½ å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"}
            ]
            st.rerun()
        
        if "messages" not in st.session_state:
            st.session_state.messages = [
                {"role": "assistant", "content": "ä½ å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"}
            ]

        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for msg in st.session_state.messages:
            st.chat_message(msg["role"]).write(msg["content"])

    async def process_query(self, query: str) -> AsyncGenerator[str, None]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆå“åº”"""
        
        # åˆå§‹åŒ–å˜é‡
        import json
        intent_content = ""
        reasoning_content = ""
        think_mode = False  # ç¡®ä¿æ¯æ¬¡æ–°æŸ¥è¯¢éƒ½é‡ç½®
        reasoning_emitted = False  # ç¡®ä¿æ¯æ¬¡æ–°æŸ¥è¯¢éƒ½é‡ç½®
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": query})
        
        # å‡†å¤‡å¯¹è¯ä¸Šä¸‹æ–‡
        messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"}]
        messages.extend([{"role": msg["role"], "content": msg["content"]} 
                        for msg in st.session_state.messages[:-1]])
        
#         test_img = """
# <div style="padding: 10px;">
#     <img src="https://www.baidu.com/img/flexible/logo/pc/result.png" width="100">
#     <p>æµ‹è¯•å›¾ç‰‡æ˜¾ç¤º</p>
# </div>
# """
#         yield test_img  # å¦‚æœè¿™ä¸ªèƒ½æ˜¾ç¤ºï¼Œè¯´æ˜æ˜¯è·¯å¾„é—®é¢˜
        # æµå¼å¤„ç†å“åº”
        
        async for chunk in self.llm.web_search(query, context=messages, tools={'è”ç½‘æœç´¢'}):
            reasoning_content=''
            # æ£€æŸ¥æ˜¯å¦æœ‰ stop_reason å­—æ®µ
            if not hasattr(chunk, 'choices') or not chunk.choices:
                continue
         
            if hasattr(chunk.choices[0].delta, 'reasoning_content')and chunk.choices[0].delta.reasoning_content:
                        reasoning_content += chunk.choices[0].delta.reasoning_content
                        if not reasoning_emitted :
                            start_label =  """<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
<b>æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹</b>
</div>"""
                            yield f"{start_label}"
                            reasoning_emitted= True
                        md_content = f"{chunk.choices[0].delta.reasoning_content}"
                        yield md_content
            if chunk.choices[0].delta.content:
                        if chunk.choices[0].delta.content=="<think>":
                                think_mode=True
                                start_label =  """<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
<b>æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹</b>
</div>"""                       
                                yield f"{start_label}"
                        elif chunk.choices[0].delta.content == "</think>":
                            
                                think_mode = False
                        elif think_mode:  
                                yield f"{chunk.choices[0].delta.content}"
                        else:      
                             intent_content+= chunk.choices[0].delta.content
          
        action_name,keywords,engines=self.llm.contains_web_search_instruction(intent_content)
        print(f"action_name: {action_name}",f"keywords: {keywords}")
        web_search_mes=messages.copy()
        if keywords and action_name:
            # 2.æœç´¢
            engine_logos_html = ""
            #print(  f"searching for {keywords} using {engines}")
            if not isinstance(engines, list):
                engines = [engines]
            for engine in engines:
                logo_path = self.llm.get_engine_logo(engine)
                with open(logo_path, "rb") as f:
                    logo_bytes = f.read()
                import base64
                logo_base64 = base64.b64encode(logo_bytes).decode()
                engine_logos_html += f"<img src='data:image/png;base64,{logo_base64}' width='40' style='margin-right: 8px;'>"
             
                
            use_label=f"""<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
<b>ä½¿ç”¨å·¥å…·:è”ç½‘æœç´¢</b>{engine_logos_html}
</div>"""
            yield use_label
             # å•ç‹¬æ¸²æŸ“logoéƒ¨åˆ†
    #         if engine_logos_html:
    #             yield f"""<div style='margin: 10px 0;'>
    # {engine_logos_html}
    # </div>"""
            search_results,ref= await self.llm.search_with_context(keywords,enable_reflection=True,enabled_engines=engines)
           # yield f'{engine_logos_html if engine_logos_html else ""}'
            # 3.ç”Ÿæˆæœç´¢ç»“æœçš„æ€»ç»“æç¤º        
            summary_prompt = self.llm.get_summary_prompt(keywords,search_results)
        
            # ç®€åŒ–æœç´¢ç»“æœå±•ç¤º
            search_results_display = ""
            for i, result in enumerate(search_results, 1):
                search_results_display += f"""
{i}. **{result.get('title', 'æ— æ ‡é¢˜')}**  
ğŸ”— {result.get('href', '')}\n
"""
            yield search_results_display

            web_search_mes=messages[-2:].copy() if len(messages) >2 else messages.copy()
            web_search_mes[-1]={
                    "role": "user",
                    "content": summary_prompt
                }
            think_mode=False
            reasoning_emitted=False
            async for chunk in self.llm.call_llm_stream(messages=web_search_mes): 
                            if not hasattr(chunk, 'choices') or not chunk.choices:
                                        continue
                            elif hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content :
                                reasoning_emitted=True
                                html_content = chunk.choices[0].delta.reasoning_content 
                                yield f"{json.dumps(html_content, ensure_ascii=False)}\n"
                                
                            elif hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content :
                                    if reasoning_emitted:
                                        reasoning_emitted=False
                                        think_mode = False
                                        label="""<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
    <b>æ€è€ƒç»“æŸï¼šå‡†å¤‡è¾“å‡ºç»“æœ</b>
    </div>"""
                                        yield f"{label}"
                                
                                    elif chunk.choices[0].delta.content=="<think>":

                                        think_mode=True
                                        labelx="""<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
    <b>æ ¹æ®æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œè¿›è¡Œå¦‚ä¸‹åˆ†æå’Œæ•´ç†</b>
    </div>"""
                                        yield f"{labelx}\n"
                                
                                    elif chunk.choices[0].delta.content == "</think>":
                                      
                                        label="""<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
    <b>æ€è€ƒç»“æŸï¼šå‡†å¤‡è¾“å‡ºç»“æœ</b>
    </div>"""
                                        yield f"{label}"
                                        think_mode = False
                                    elif  not think_mode  or not  reasoning_emitted:   
                                        yield {"type": "full_results", "data": chunk.choices[0].delta.content}
        else:
                think_mode=False
                reasoning_emitted=False
                messages.append({"role": "user", "content": query})
             
                async for chunk in self.llm.call_llm_stream(messages=messages): 
                        
                            if not hasattr(chunk, 'choices') or not chunk.choices:
                                         continue
                            elif hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content :
                                reasoning_emitted=True
                                html_content = chunk.choices[0].delta.reasoning_content 
                                yield f"{json.dumps(html_content, ensure_ascii=False)}\n"
                                
                            elif hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content :
                                    if reasoning_emitted:
                                        reasoning_emitted=False
                                        think_mode = False
                                        label="""<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
    <b>æ€è€ƒç»“æŸï¼šå‡†å¤‡è¾“å‡ºç»“æœ</b>
    </div>"""
                                        yield f"{label}"
                           
                                    elif chunk.choices[0].delta.content == "<think>":
                                        yield "\n"
                                    elif chunk.choices[0].delta.content == "</think>":
                                        label="""<div style='color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;'>
    <b>æ€è€ƒç»“æŸï¼šå‡†å¤‡è¾“å‡ºç»“æœ</b>
    </div>"""
                                        yield f"{label}"
                                        think_mode = False
                                    elif  not think_mode  or not  reasoning_emitted:   
                                        yield {"type": "full_results", "data": chunk.choices[0].delta.content}
                         
    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        if query := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            st.chat_message("user").write(query)
            
            # åˆ›å»ºå ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
            response_placeholder = st.empty()
            full_response = ""
            save_content=""
            # å¤„ç†å¹¶æ˜¾ç¤ºå“åº”
            async def stream_response():
                nonlocal full_response
                nonlocal save_content
                async for chunk in self.process_query(query):
                    #full_response += chunk
                    if isinstance(chunk, dict):
                        if chunk.get("type") == "full_results":
                            
                            save_content += chunk["data"]  # ä¿å­˜å®Œæ•´ç»“æœ
                            full_response += chunk["data"] 
                       
                    else:
                        
                        full_response += chunk
                    response_placeholder.markdown(full_response,unsafe_allow_html=True)
                #print("save_content:", save_content)
                # æ·»åŠ å®Œæ•´å“åº”åˆ°å†å²
                st.session_state.messages.append({"role": "assistant", "content":  save_content})
            
            asyncio.run(stream_response())

if __name__ == "__main__":
    app = ChatApp()
    app.run()